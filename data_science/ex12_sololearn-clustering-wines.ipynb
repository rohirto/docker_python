{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T18:55:07.332314Z","iopub.execute_input":"2023-09-21T18:55:07.332648Z","iopub.status.idle":"2023-09-21T18:55:07.740195Z","shell.execute_reply.started":"2023-09-21T18:55:07.332619Z","shell.execute_reply":"2023-09-21T18:55:07.739218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clustering is a type of unsupervised learning that allows us to find groups of similar objects, objects that are more related to each other than to the objects in other groups.\n\nUnsupervised because labels are missing - we dont know what to make of data\n\nExamples of business use cases include the grouping of documents, music, and movies based on their contents, or finding customer segments based on purchase behavior as a basis for recommendation engines.\n\nThe goal of clustering is to separate the data into groups, or clusters, with more similar traits to each other than to the data in the other clusters\n\nDifferent Types of Clustering Algorithms - Major ones: \n\nCentroid based models - each cluster is represented by a single mean vector (e.g., k-means),\nConnectivity based models - built based on distance connectivity (e.g., hierarchical clustering)\nDistribution based models - built using statistical distributions (e.g., Gaussian mixtures)\nDensity based models - clusters are defined as dense areas (e.g., DBSCAN)\n","metadata":{}},{"cell_type":"markdown","source":"K means Clustering \n\nAssuming that there are n data points, the algorithm works as follows:\n\nStep 1: initialization - pick k random points as cluster centers, called centroids\n\nStep 2: cluster assignment - assign each data point to its nearest centroid based on its distance to each centroid, and that forms k clusters\n\nStep 3: centroid updating - for each new cluster, calculate its centroid by taking the average of all the points assigned to the cluster\n\nStep 4: repeat steps 2 and 3 until none of cluster assignments change, or it reaches the maximum number of iterations\n\nHowever, it is difficult to predict the number of clusters, it can get stuck in local optimums, and it can perform poorly when the clusters are of varying sizes and density.","metadata":{}},{"cell_type":"markdown","source":"How do we calculate the distance in k-means algorithm?\n\nThrough euclidean distance - pythagoras theorem \nThere are other distance metrics, such as Manhattan distance, cosine distance, etc. The choice of the distance metric depends on the data.","metadata":{}},{"cell_type":"markdown","source":"Wine Data is going be used here in this tutorial\n\nresult of a chemical analysis of wines grown in a particular region in Italy.\ngoal is to try to group similar observations together and determine the number of possible clusters.\n\nThe analysis reported the quantities of 13 constituents from 178 wines: \n1. alcohol, 2. malic acid, 3. ash, 4. alcalinity of ash, 5. magnesium, 6. total phenols, 7. flavanoids, 8. nonflavanoid phenols, 9. proanthocyanins, 10. color intensity, 11. hue, 12. od280/od315 of diluted wines, and 13. proline.\n","metadata":{}},{"cell_type":"code","source":"# Loading the data and evaluating it\n\nimport numpy as numpy \nimport pandas as pd\nfrom sklearn.datasets import load_wine\n\ndata = load_wine()\nwine = pd.DataFrame(data.data, columns=data.feature_names)\nprint(wine.shape)\nprint(wine.columns)\n\n# Basic statics of first 3 features\nprint(wine.iloc[:,:3].describe())","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:58:02.003831Z","iopub.execute_input":"2023-09-21T18:58:02.004254Z","iopub.status.idle":"2023-09-21T18:58:02.039007Z","shell.execute_reply.started":"2023-09-21T18:58:02.004207Z","shell.execute_reply":"2023-09-21T18:58:02.037872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the data \n\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(wine.iloc[:,[0,5]])\nplt.savefig(\"plot.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:00:45.568282Z","iopub.execute_input":"2023-09-21T19:00:45.568802Z","iopub.status.idle":"2023-09-21T19:00:46.300906Z","shell.execute_reply.started":"2023-09-21T19:00:45.568762Z","shell.execute_reply":"2023-09-21T19:00:46.299774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we don’t know the ground truth, we look into the scatter plots to come up with a reasonable candidate for k, the number of clusters. \n\nThere seem to be roughly three subgroups. \n\nRemember that there are no right or wrong answers for the number of subgroups. \n\nIn the real world data, rarely do we find clear clusters; but we come up with our best educated guess. \nFor example, in the scatter plot above, there seem to be three subgroups.","metadata":{}},{"cell_type":"markdown","source":"Pre-processing: Standardization\n\nAfter examining all the pairs of scatter plot, we pick two features to better illustrate the algorithm: alcohol and total_phenols, whose scatterplot also suggests three subclusters.\n\nUnlike any supervised learning models, in general, **unsupervised machine learning models do not require to split data into training and testing sets** since there is no ground truth to validate the model.\n\nHowever, **centroid-based algorithms require one pre-processing step because k-means works better on data where each attribute is of similar scales.**\n\nOne way to achieve this is to standardize the data; mathematically:\nz = (x - mean) / std\n\nStandardScaler under the sklearn.preprocessing makes it easy:","metadata":{}},{"cell_type":"code","source":"X = wine[['alcohol', 'total_phenols']] \n\nfrom sklearn.preprocessing import StandardScaler\n# instantiate the scaler\nscale = StandardScaler()\n# compute the mean and std to be used later for scaling\nscale.fit(X)\n\n# This are raw values \nprint(scale.mean_)\nprint(scale.scale_)\n\nprint(\"\\n\")\n\n# Now we Transform/Scale the data around 0,0\nX_scaled = scale.transform(X)\n# Let’s do a sanity check to see if each feature is centered at 0 and has a std of 1:\nprint(X_scaled.mean(axis=0))  # Mean comes down almost to zero, as data is scaled around 0,0\nprint(X_scaled.std(axis=0))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:19:13.743568Z","iopub.execute_input":"2023-09-21T19:19:13.743948Z","iopub.status.idle":"2023-09-21T19:19:13.757212Z","shell.execute_reply.started":"2023-09-21T19:19:13.743920Z","shell.execute_reply":"2023-09-21T19:19:13.756089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Modelling\n\nK-means Modeling\n\nwe follow instantiate / fit / predict workflow\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n# instantiate the model\nkmeans = KMeans(n_clusters=3)\n# fit the model\nkmeans.fit(X_scaled)\n# make predictions\ny_pred = kmeans.predict(X_scaled)\nprint(y_pred)\n\nprint(\"\\n\")\nprint(kmeans.cluster_centers_) # See the cluster centres\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:31:39.388031Z","iopub.execute_input":"2023-09-21T19:31:39.388477Z","iopub.status.idle":"2023-09-21T19:31:39.418347Z","shell.execute_reply.started":"2023-09-21T19:31:39.388442Z","shell.execute_reply":"2023-09-21T19:31:39.417192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Data Visualization \"\"\"\n# plot the scaled data\nplt.scatter(X_scaled[:,0],\n            X_scaled[:,1],\n            c= y_pred)\n# identify the centroids\nplt.scatter(kmeans.cluster_centers_[:, 0],\n            kmeans.cluster_centers_[:, 1],\n            marker=\"*\",\n            s = 250,\n            c = [0,1,2],\n            edgecolors='k')\n\nplt.xlabel('alcohol'); plt.ylabel('total phenols')\nplt.title('k-means (k=3)')\nplt.savefig(\"plot.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:34:05.567723Z","iopub.execute_input":"2023-09-21T19:34:05.568120Z","iopub.status.idle":"2023-09-21T19:34:05.968288Z","shell.execute_reply.started":"2023-09-21T19:34:05.568091Z","shell.execute_reply":"2023-09-21T19:34:05.967229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" K-means divides wines into three groups: \n1. low alcohol but high total phenols (upper right in green), \n2. high alcohol and high total phenols (upper left in yellow), and \n3. low total phenols (bottom in purple)\n\nFor any new wine with the chemical report on alcohol and total phenols, we now can classify it based on its distance to each of the centroids. \n\nSuppose that there is new wine with alcohol at 13 and total phenols at 2.5, let’s predict which cluster the model will assign the new wine to.","metadata":{}},{"cell_type":"code","source":"\nX_new = np.array([[13, 2.5]])\nX_new_scaled = scale.transform(X_new)\n\nprint(kmeans.predict(X_new_scaled))\n\nprint(kmeans.cluster_centers_[kmeans.predict(X_new_scaled)])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:46:15.668163Z","iopub.execute_input":"2023-09-21T19:46:15.669267Z","iopub.status.idle":"2023-09-21T19:46:15.677643Z","shell.execute_reply.started":"2023-09-21T19:46:15.669215Z","shell.execute_reply":"2023-09-21T19:46:15.676556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One major shortcoming of k-means is that the random initial guess for the centroids can result in bad clustering, and k-means++ algorithm addresses this obstacle by specifying a procedure to initialize the centroids before proceeding with the standard k-means algorithm. In scikit-learn, the initialization mechanism is set to k-means++, by default.","metadata":{}},{"cell_type":"markdown","source":"Optimal K: The Elbow method\n\nAs shown, k-means will be happy to divide the dataset into any integer number of clusters, ranging from 1, an extreme case where all data points belong to one big cluster, to n, another extreme case where each data point is its own cluster.\n\nSo which one should we choose, 2, or 3, or 4 for the wines?\n\nIntuitively, k-means problem partitions n data points into k tight sets such that the data points are closer to each other than to the data points in the other clusters. \n\nthe tightness can be measured as the sum of squares of the distance from data point to its nearest centroid, or inertia.\n\nn scikit-learn, it is stored as inertia_, e.g. when k = 2, the distortion is 185:","metadata":{}},{"cell_type":"code","source":"# Inertia when k=2\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X_scaled)\nprint(kmeans.inertia_)\n\n# inertia when k = 3\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X_scaled)\nprint(kmeans.inertia_)\n\n# We plot the inertia for different values of k:\ninertia = []\nfor i in np.arange(1, 11):\n    km = KMeans(\n        n_clusters=i\n    )\n    km.fit(X_scaled)\n    inertia.append(km.inertia_)\n\n# plot\nplt.plot(np.arange(1, 11), inertia, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:53:01.023648Z","iopub.execute_input":"2023-09-21T19:53:01.024072Z","iopub.status.idle":"2023-09-21T19:53:01.525903Z","shell.execute_reply.started":"2023-09-21T19:53:01.024037Z","shell.execute_reply":"2023-09-21T19:53:01.524755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the plot shows, the inertia decreases as the number of clusters increases. The optimal k should be where the inertia no longer decreases as rapidly.\n\nFor example, k=3 seems to be optimal,\n\nas we increase the number of clusters from 3 to 4, the decrease in inertia slows down significantly, compared to that from 2 to 3. \n\nThis approach is called elbow method (can you see why? - elbow at 3). It is a useful graphical tool to estimate the optimal k in k-means.","metadata":{}},{"cell_type":"markdown","source":"Modelling with More Features\n\nPreviously to build kmeans models, we used two (out of thirteen) features: alcohol and total phenols.\n\nThe choice is random and it is easy to visualize the results.\n\nHowever, can we use more features, for example all of them? Why not? Let’s try it.","metadata":{}},{"cell_type":"code","source":"X = wine # Using all the features of dataset\n\nscale = StandardScaler()\nscale.fit(X)\nX_scaled = scale.transform(X)\n\n\nfrom sklearn.cluster import KMeans\n# calculate distortion for a range of number of cluster\ninertia = []\nfor i in np.arange(1, 11):\n    km = KMeans(\n        n_clusters=i \n        )\n    km.fit(X_scaled) \n    inertia.append(km.inertia_)\nplt.plot(np.arange(1, 11), inertia, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.title(\"all features\")\nplt.savefig(\"plot.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:57:49.503216Z","iopub.execute_input":"2023-09-21T19:57:49.503658Z","iopub.status.idle":"2023-09-21T19:57:50.153694Z","shell.execute_reply.started":"2023-09-21T19:57:49.503623Z","shell.execute_reply":"2023-09-21T19:57:50.152597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly we spot that the inertia no longer decreases as rapidly after k = 3. We then finalize the model by setting n_clusters = 3 and obtain the predictions.","metadata":{}},{"cell_type":"code","source":"k_opt = 3\nkmeans = KMeans(k_opt)\nkmeans.fit(X_scaled)\ny_pred = kmeans.predict(X_scaled)\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:58:34.848578Z","iopub.execute_input":"2023-09-21T19:58:34.848969Z","iopub.status.idle":"2023-09-21T19:58:34.877854Z","shell.execute_reply.started":"2023-09-21T19:58:34.848937Z","shell.execute_reply":"2023-09-21T19:58:34.876863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compared to the predictions using only two features, the two models produce very similar results.\n\nFor instance, the first 21 wines are predicted to belong to the same cluster from both models, so are the last 19 wines.\n\nIn fact, only 13 out of 178 wines were clustered differently by the two models.\n\nwhich model is better? Recall that clustering is an unsupervised learning method, which indicates that we don’t know the ground truth of the labels. Thus it is difficult, if not impossible, to determine that the model with 2 features is more accurate in grouping wines than the one with all 13 features, or vice versa.\n\nWhich model, in other words which features, should you choose is often determined by external information.\n\n\nIn practice, the features are often chosen by the collaboration between data scientists and domain knowledge experts.","metadata":{}}]}