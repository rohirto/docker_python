{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Plotiing\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T13:32:04.693900Z","iopub.execute_input":"2023-09-21T13:32:04.694303Z","iopub.status.idle":"2023-09-21T13:32:04.707283Z","shell.execute_reply.started":"2023-09-21T13:32:04.694272Z","shell.execute_reply":"2023-09-21T13:32:04.706414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Iris Flower Data set \nIris is a flower wih following features\n![](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)","metadata":{}},{"cell_type":"code","source":"\"\"\" Step 1 Data exploration \"\"\"\n# Check if running on Kaggle Notebook (you can define your own check)\nif 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n    # Load dataset from Kaggle\n    iris = pd.read_csv('/kaggle/input/sololearn-iris/iris.csv')\nelse:\n    # Load dataset locally\n    iris = pd.read_csv('/workspaces/docker_python/data_science/datasets/iris.csv')\n   \n\n# Print the shape of dataset\nprint(iris.shape)\n\n# Print the head\nprint(iris.head())\n\n# Check the summary statistics\nprint(iris.describe())\n\n# 'id' column is of no use thus we drop it\niris.drop('id', axis=1, inplace=True)\n\nprint(iris.head())\n\n# To view the classes of categorical variable, 2 Methods\nprint(iris.groupby('species').size())\n\nprint(iris['species'].value_counts())\n# The above dataset is a balanced dataset, its opposite being unbalanced dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T13:32:04.708819Z","iopub.execute_input":"2023-09-21T13:32:04.710010Z","iopub.status.idle":"2023-09-21T13:32:04.744539Z","shell.execute_reply.started":"2023-09-21T13:32:04.709910Z","shell.execute_reply":"2023-09-21T13:32:04.743519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Data Visualization \"\"\"\n\n# Univariate Plots of Features\niris.hist()\nplt.show()\n\n# Multivariate Plotiing\n# build a dict mapping species to an integer code\ninv_name_dict = {'iris-setosa': 0,\n'iris-versicolor': 1,\n'iris-virginica': 2}\n\n# build integer color code 0/1/2\ncolors = [inv_name_dict[item] for item in iris['species']] # assigns the color code 0/1/2 to the cloumn 'species'\nprint(colors)\n# scatter plot of sepals\nscatter = plt.scatter(iris['sepal_len'], iris['sepal_wd'], c = colors)\nplt.xlabel('sepal length (cm)')\nplt.ylabel('sepal width (cm)')\n## add legend\nplt.legend(handles=scatter.legend_elements()[0],\nlabels = inv_name_dict.keys())\nplt.savefig(\"plot.png\")\nplt.show()\n\n# scatter plot of petals\n# scatter plot\nscatter = plt.scatter(iris['petal_len'], iris['petal_wd'],c = colors)\nplt.xlabel('petal length (cm)')\nplt.ylabel('petal width (cm)')\n# add legend\nplt.legend(handles= scatter.legend_elements()[0],\n  labels = inv_name_dict.keys())\nplt.show()\n\n# Scatter matrix\npd.plotting.scatter_matrix(iris)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T13:32:04.745847Z","iopub.execute_input":"2023-09-21T13:32:04.746406Z","iopub.status.idle":"2023-09-21T13:32:07.293496Z","shell.execute_reply.started":"2023-09-21T13:32:04.746364Z","shell.execute_reply":"2023-09-21T13:32:07.292368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"K nearest neighbors\nK nearest neighbors (knn) is a supervised machine learning model that takes a data point, looks at its 'k' closest labeled data points, and assigns the label by a majority vote.\n\nHere we see that changing k could affect the output of the model. In knn, k is a hyperparameter. A hyperparameter in machine learning is a parameter whose value is set before the learning process begins.\n\nFor example, in the figure below, there are two classes: blue squares and red triangles. What label should we assign to the green dot, with unknown label, based on the 3nn algorithm, i.e., when k is 3? Of the 3 closest data points from the green dot (solid line circle), two are red triangles and one is blue square, thus the green dot is predicted to be a red triangle. If k is 5 (dashed line circle), it is then classified as a blue square (3 blue squares versus 2 red triangles, blue squares are the majority).\n\n![](https://lecontent.sololearn.com/material-images/00000d0d00000445531d0000fe0e0000_data%20visualization.png)","metadata":{}},{"cell_type":"code","source":"\"\"\" Modelling \"\"\"\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Set the features and Target\nX = iris[['petal_len', 'petal_wd']]\ny = iris['species']\n\n# Split the test data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1,stratify=y)\n\n# Print the categorical count of train and test data \nprint(y_train.value_counts())\nprint(y_test.value_counts())\n\n## instantiate \nknn = KNeighborsClassifier(n_neighbors=5)\n\n## fit \nprint(knn.fit(X_train, y_train))\nprint(\"\\n\")\n\n## Predict on test dataset\ny_pred = knn.predict(X_test)\nprint(y_pred[:5])\nprint(\"\\n\")\nprint(y_pred[10:12]) # See the prediction on 11th and 12th elements\n\n## Probablity Prediction - Doesnot label class output but probability of being classified to that class\ny_pred_prob = knn.predict_proba(X_test)\nprint(y_pred_prob[10:12]) # predict for 11, 12 -> for 10th ele [1. 0. 0.] \n                          # Which means probability of the 11th flower being\n                          # predicted an iris-setosa is 1, an iris-versicolor\n                          # and an iris-virginica are both 0.\n                          # For the next flower, there is a 20% chance that it\n                          # would be classified as iris-versicolor but 80% chance to be iris-virginica.\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T13:32:07.295593Z","iopub.execute_input":"2023-09-21T13:32:07.295932Z","iopub.status.idle":"2023-09-21T13:32:07.326529Z","shell.execute_reply.started":"2023-09-21T13:32:07.295902Z","shell.execute_reply":"2023-09-21T13:32:07.325284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Above Cell, explanation: \nProbability Prediction: For example, the probability of the 11th flower being predicted an iris-setosa is 1, an iris-versicolor and an iris-virginica are both 0. For the next flower, there is a 20% chance that it would be classified as iris-versicolor but 80% chance to be iris-virginica. What it tells us is that of the five nearest neighbours of the 12th flower in the testing set, 1 is an iris-versicolor, the rest 4 are iris-virginica.","metadata":{}},{"cell_type":"code","source":"\"\"\" Model Evaluation \"\"\"\n# To measure Accuracy\n\n# Check how many correct predictions were made\nprint((y_pred==y_test.values).sum()) # Correct Predictions\nprint(y_test.size) # Total Test size\n\n# Effiecency \nprint((y_pred==y_test.values).sum()/y_test.size)\nprint(\"\\n\")\n# Effiecency r_score\nprint(knn.score(X_test, y_test))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T13:32:07.328028Z","iopub.execute_input":"2023-09-21T13:32:07.328463Z","iopub.status.idle":"2023-09-21T13:32:07.345078Z","shell.execute_reply.started":"2023-09-21T13:32:07.328422Z","shell.execute_reply":"2023-09-21T13:32:07.343940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above code shows our model made 1 mistake\n\nClassification accuracy alone can be misleading if there is an unequal number of observations in each class or if there are more than two classes in the dataset.\n\nCalculating a confusion matrix will provide a better idea of what the classification is getting right and what types of errors it is making.\n\nWhat is a confusion matrix? It is a summary of the counts of correct and incorrect predictions, broken down by each class.\nIn classifying the iris, we can use confusion_matrix() under module sklearn.metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \nprint(confusion_matrix(y_test, y_pred))\n\n# Above operation can be graphically done as \nfrom sklearn.metrics import ConfusionMatrixDisplay\nmat = confusion_matrix(y_test, y_pred, labels = ['iris-setosa', 'iris versicolor', 'iris-virginica']) \ndisp = ConfusionMatrixDisplay( confusion_matrix=mat, display_labels=['iris-setosa', 'iris versicolor', 'iris-virginica'])\ndisp.plot()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T13:34:59.752282Z","iopub.execute_input":"2023-09-21T13:34:59.752717Z","iopub.status.idle":"2023-09-21T13:35:00.093184Z","shell.execute_reply.started":"2023-09-21T13:34:59.752681Z","shell.execute_reply":"2023-09-21T13:35:00.092052Z"},"trusted":true},"execution_count":null,"outputs":[]}]}